{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Info from Youtube Videos\n",
    "### Multithreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using urllib2\n",
    "use this for westeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import json\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import bs4\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_url = 'https://www.youtube.com/watch?v=e7t4pTNZshA'\n",
    "page_html= urllib2.urlopen(my_url).read()\n",
    "soup_out = bs4.BeautifulSoup(page_html)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# soup_out\n",
    "# page_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# page_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# page= \"\".join(page_html.split())\n",
    "# page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_meta(soup_base,prop_name,attribute_name):\n",
    "    temp = soup_base.find_all('meta',{prop_name:attribute_name})\n",
    "    for element in temp:\n",
    "        out =(element.attrs[\"content\"])    \n",
    "    return out\n",
    "        \n",
    "def extract_video_info(my_url=None,result_flag=True):\n",
    "    #     print(my_url)\n",
    "    page_html= urllib2.urlopen(my_url).read()\n",
    "    sleep(0.002)\n",
    "    #     print(page_html)\n",
    "    soup_out = bs4.BeautifulSoup(page_html)    \n",
    "    out_title=extract_meta(soup_out,\"name\",\"title\").encode('utf-8')\n",
    "    out_keys=extract_meta(soup_out,\"name\",\"keywords\").encode('utf-8')\n",
    "\n",
    "    temp = soup_out.find_all('script',{'type':'application/ld+json'})[0]\n",
    "    description=extract_meta(soup_out,\"itemprop\",\"description\").encode('utf-8')\n",
    "    category=extract_meta(soup_out,\"itemprop\",\"genre\").encode('utf-8')\n",
    "    viewcount=extract_meta(soup_out,\"itemprop\",\"interactionCount\").encode('utf-8')\n",
    "    twitter_description=extract_meta(soup_out,\"name\",\"twitter:description\").encode('utf-8')\n",
    "    channelID=extract_meta(soup_out,\"itemprop\",\"channelId\").encode('utf-8')\n",
    "    duration=extract_meta(soup_out,\"itemprop\",\"duration\").encode('utf-8')\n",
    "    videoID=extract_meta(soup_out,\"itemprop\",\"videoId\").encode('utf-8')\n",
    "\n",
    "    for string in temp.strings:\n",
    "        a=json.loads(string)\n",
    "        author = a['itemListElement'][0]['item']['name'].encode('utf-8')    \n",
    "    if result_flag:\n",
    "        result = {\"title\": out_title, \"keywords\": out_keys,  \\\n",
    "                 \"author\": author, \"category\": category,     \\\n",
    "                 \"viewcount\": viewcount, \"description\": description, \\\n",
    "                 \"twitter_desc\": twitter_description,         \\\n",
    "                 \"channelID\":channelID, \"duration\":duration,  \\\n",
    "                 \"videoID\":videoID}\n",
    "        \n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        \n",
    "        return out_title, out_keys,author, \\\n",
    "            category,viewcount,description, \\\n",
    "            twitter_description,channelID,duration,videoID\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hattie McDaniel winning Best Supporting Actress', 'Oscars, Academy, Awards, Hattie, McDaniel, Best, Supporting, Actress, Gone, with, the, Wind', 'Oscars', 'Entertainment', '374332', 'Fay Bainter presenting Hattie McDaniel with the Oscar\\xc2\\xae for Best Supporting Actress for her performance in \"Gone With The Wind\" at the 12th Academy Awards\\xc2\\xae in...', 'Fay Bainter presenting Hattie McDaniel with the Oscar\\xc2\\xae for Best Supporting Actress for her performance in \"Gone With The Wind\" at the 12th Academy Awards\\xc2\\xae in...', 'UCb-vZWBeWA5Q2818JmmJiqQ', 'PT1M41S', 'e7t4pTNZshA')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    t,k,a,c,v,d,td,chid,duration,videoID = extract_video_info('https://www.youtube.com/watch?v=e7t4pTNZshA',result_flag=False)\n",
    "    print(t,k,a,c,v,d,td,chid,duration,videoID)\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 read csv list of url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_data = pd.read_csv('./sample_vids.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 call extract_video_info to loop on list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DIDITH ROMERO in SA DILA NG APOY', 'TAGALOG, MOVIE', 'pilipinomovies', 'Film & Animation', '220546', 'SA DILA NG APOY,1987 * BOLD AND PENE MOVIE* CHECK MY BLOG--http://ratedxxxmovies.blogspot.com', 'SA DILA NG APOY,1987 * BOLD AND PENE MOVIE* CHECK MY BLOG--http://ratedxxxmovies.blogspot.com', 'UCh-eblga9rZwmUTuft22rtw', 'PT3M32S', '0XV3JQBX9sM')\n",
      "('M\\xc3\\xa1s Sabe el Diablo | Jencarlos Canela y Gaby Espino hacen el amor | Telemundo', 'Jencarlos Canela, Gaby Espino, TVTelemundo, TelemundoChannel', 'Telemundo FANS', 'Entertainment', '2791458', 'Jencarlos Canela y Gaby Espino en una de las escenas m\\xc3\\xa1s calientes de todos los tiempos en la televisi\\xc3\\xb3n.', 'Jencarlos Canela y Gaby Espino en una de las escenas m\\xc3\\xa1s calientes de todos los tiempos en la televisi\\xc3\\xb3n.', 'UCcgrOOB6iaytX1nG0UFmgTw', 'PT1M34S', 'EDLwppTAdW8')\n",
      "('Realistic Earthquake Sound Effect [High Quality]', 'sound, effect, hd, 320kbs, 320, recording, wav, mp3, fx, ambience, ambiance, background, noise, audio, high, quality, free, copyright, royalty, creative, com...', 'SoundEffectsFactory', 'Entertainment', '137641', 'Lots of bass in this one! Very realistic! Subscribe for more Sounds! http://bit.ly/TiIuFz Extras Channel! http://bit.ly/1iQgZ0q My Facebook! http://www.faceb...', 'Lots of bass in this one! Very realistic! Subscribe for more Sounds! http://bit.ly/TiIuFz Extras Channel! http://bit.ly/1iQgZ0q My Facebook! http://www.faceb...', 'UCYIxR_86Ck0sCL26eVuumvQ', 'PT0M22S', 'mgLBmLoL2Aw')\n",
      "('Namir ... Ya Haly - Video Clip | \\xd9\\x86\\xd9\\x85\\xd9\\x8a\\xd8\\xb1 ... \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a - \\xd9\\x81\\xd9\\x8a\\xd8\\xaf\\xd9\\x8a\\xd9\\x88 \\xd9\\x83\\xd9\\x84\\xd9\\x8a\\xd8\\xa8', 'Arabic, Arab, Tarab, Music, Song, Rotana, \\xd8\\xb1\\xd9\\x88\\xd8\\xaa\\xd8\\xa7\\xd9\\x86\\xd8\\xa7, \\xd8\\xb9\\xd8\\xb1\\xd8\\xa8\\xd9\\x8a, \\xd8\\xa3\\xd8\\xba\\xd9\\x86\\xd9\\x8a\\xd9\\x87, \\xd8\\xb7\\xd8\\xb1\\xd8\\xa8, \\xd9\\x85\\xd8\\xb2\\xd9\\x8a\\xd9\\x83\\xd8\\xa7, \\xd9\\x83\\xd9\\x84\\xd9\\x8a\\xd8\\xa8, \\xd9\\x81\\xd9\\x8a\\xd8\\xaf\\xd9\\x8a\\xd9\\x88, \\xd8\\xa7\\xd8\\xb3\\xd9\\x85\\xd8\\xb9, \\xd9\\x86\\xd9\\x85\\xd9\\x8a\\xd8\\xb1, \\xd9\\x86\\xd9\\x85\\xd8\\xa7\\xd8\\xb1, \\xd8\\xb9\\xd8\\xb1\\xd8\\xa7\\xd9\\x82, \\xd9\\x8a\\xd8\\xa7, \\xd9\\x87\\xd9\\x84\\xd9\\x8a, \\xd9\\x87\\xd9\\x84\\xd9\\x89, namir, namer, namyr, ya, yah, h...', 'Rotana', 'Music', '3346281', '\\xd9\\x83\\xd9\\x84\\xd9\\x85\\xd8\\xa7\\xd8\\xaa : \\xd9\\x83\\xd8\\xa7\\xd8\\xb8\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a , \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xa7\\xd9\\x86 : \\xd9\\x86\\xd9\\x85\\xd9\\x8a\\xd8\\xb1 \\xd9\\x83\\xd9\\x84\\xd9\\x85\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xba\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a \\xd9\\x84\\xd8\\xa7 \\xd8\\xaa\\xd8\\xad\\xd8\\xb1\\xd8\\xac\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a \\xd9\\x84\\xd8\\xa7 \\xd8\\xaa\\xd8\\xb9\\xd8\\xb0\\xd8\\xa8\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x81\\xd9\\x8a \\xd8\\xb4\\xd9\\x82\\xd8\\xa7\\xd9\\x8a \\xd9\\x88 \\xd9\\x81\\xd9\\x8a \\xd8\\xb8\\xd9\\x86\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x85\\xd9\\x86 \\xd8\\xb9\\xd8\\xb0\\xd8\\xa7\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xa8 \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a \\xd9\\x84\\xd9\\x88 \\xd8\\xaa\\xd9\\x81\\xd9\\x87\\xd9\\x85\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x84\\xd8\\xad\\xd8\\xb8\\xd9\\x87 \\xd9\\x85\\xd8\\xa7 \\xd9\\x86\\xd8\\xa7\\xd9\\x85\\xd8\\xaa \\xd8\\xb9\\xd9\\x8a\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x88\\xd9\\x87\\xd9\\x88...', '\\xd9\\x83\\xd9\\x84\\xd9\\x85\\xd8\\xa7\\xd8\\xaa : \\xd9\\x83\\xd8\\xa7\\xd8\\xb8\\xd9\\x85 \\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a , \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xa7\\xd9\\x86 : \\xd9\\x86\\xd9\\x85\\xd9\\x8a\\xd8\\xb1 \\xd9\\x83\\xd9\\x84\\xd9\\x85\\xd8\\xa7\\xd8\\xaa \\xd8\\xa7\\xd9\\x84\\xd8\\xa3\\xd8\\xba\\xd9\\x86\\xd9\\x8a\\xd8\\xa9 \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a \\xd9\\x84\\xd8\\xa7 \\xd8\\xaa\\xd8\\xad\\xd8\\xb1\\xd8\\xac\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a \\xd9\\x84\\xd8\\xa7 \\xd8\\xaa\\xd8\\xb9\\xd8\\xb0\\xd8\\xa8\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x81\\xd9\\x8a \\xd8\\xb4\\xd9\\x82\\xd8\\xa7\\xd9\\x8a \\xd9\\x88 \\xd9\\x81\\xd9\\x8a \\xd8\\xb8\\xd9\\x86\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x85\\xd9\\x86 \\xd8\\xb9\\xd8\\xb0\\xd8\\xa7\\xd8\\xa8 \\xd8\\xa7\\xd9\\x84\\xd8\\xad\\xd8\\xa8 \\xd9\\x8a\\xd8\\xa7 \\xd9\\x87\\xd9\\x84\\xd9\\x8a \\xd9\\x84\\xd9\\x88 \\xd8\\xaa\\xd9\\x81\\xd9\\x87\\xd9\\x85\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x84\\xd8\\xad\\xd8\\xb8\\xd9\\x87 \\xd9\\x85\\xd8\\xa7 \\xd9\\x86\\xd8\\xa7\\xd9\\x85\\xd8\\xaa \\xd8\\xb9\\xd9\\x8a\\xd9\\x88\\xd9\\x86\\xd9\\x8a \\xd9\\x88\\xd9\\x87\\xd9\\x88...', 'UCNhqvQMXIgRfjAGmxQqdNRw', 'PT6M12S', 'C6l_Ogflkuk')\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "input_data\n",
    "for text in input_data[\"VIDID\"]:\n",
    "    test_url='https://www.youtube.com/watch?v=%s' %(text)\n",
    "    try:\n",
    "        \n",
    "        t,k,a,c,v,d,td,chid,duration,videoID = extract_video_info(test_url,result_flag=False)\n",
    "        print(t,k,a,c,v,d,td,chid,duration,videoID)\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942761\n",
      "379611\n"
     ]
    }
   ],
   "source": [
    "# input_data.VIDID.unique\n",
    "# input_data['VIDID'].drop_duplicates(inplace=True)\n",
    "input_data=pd.read_csv('/data/raw_data/youtube_video_id/video_id_july.csv')\n",
    "temp_vidid = input_data['vidid']\n",
    "print(input_data.vidid.count())\n",
    "\n",
    "temp_vidid.drop_duplicates(inplace=True)\n",
    "print(input_data.vidid.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the extractor for the csv of videos\n",
    "\n",
    "- Just replace the **input_csv** variable with the input csv\n",
    "- Replace **output_csv with the** output csv file\n",
    "- Modify **input_delimeter** to suit the input csv file\n",
    "- Modify **output_delimeter** if you want other delimeters\n",
    "- Replace **input_column** with the column name of the video ID\n",
    "- Set **row_start** to start the processing from this Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from multiprocessing import Process, Pool\n",
    "import time\n",
    "import urllib2\n",
    "\n",
    "def millis():\n",
    "    return int(round(time.time() * 1000))\n",
    "\n",
    "def http_get(url):\n",
    "    start_time = millis()\n",
    "    result = {\"url\": url, \"data\": urllib2.urlopen(url, timeout=5).read()}\n",
    "    print url + \" took \" + str(millis() - start_time) + \" ms\"\n",
    "    return result\n",
    "  \n",
    "urls = my_url\n",
    "pool = Pool(processes=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from urllib2 import urlopen\n",
    "from threading import Thread\n",
    "from json import JSONDecoder\n",
    " \n",
    "# Define a crawl function that retrieves data from a url and places the result in results[index]\n",
    "# The 'results' list will hold our retrieved data\n",
    "# The 'urls' list contains all of the urls that are to be checked for data\n",
    "\n",
    "def crawl(url, result, index):\n",
    "    # Keep everything in try/catch loop so we handle errors\n",
    "    try:\n",
    "        data = extract_video_info(url)\n",
    "        logging.info(\"Requested...\" + url)\n",
    "        result[index] = data\n",
    "#         result[index]['row_id'] = index\n",
    "    except:\n",
    "        logging.error('Error with URL check!')\n",
    "        result[index] = {}\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_column='vidid'\n",
    "input_data = pd.read_csv('/data/raw_data/youtube_video_id/video_id_july.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data_july=pd.read_csv('/data/raw_data/youtube_video_id/video_id_july.csv')\n",
    "input_data_june=pd.read_csv('/data/raw_data/youtube_video_id/video_id_june.csv')\n",
    "input_data_may=pd.read_csv('/data/raw_data/youtube_video_id/video_id_may.csv')\n",
    "input_data_april=pd.read_csv('/data/raw_data/youtube_video_id/video_id_april.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vidid    942761\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = input_data_july[['vidid']]\n",
    "input_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vidid    1932719\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = input_data.append(input_data_june[['vidid']])\n",
    "input_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vidid    3149011\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = input_data.append(input_data_may[['vidid']])\n",
    "input_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vidid    4399035\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = input_data.append(input_data_april[['vidid']])\n",
    "input_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    HO-R6LGuCR4\n",
       "1          mOrEe\n",
       "2    2R06ch5QYYA\n",
       "3    UsXFWqbvJZw\n",
       "4    Hf_EtwmA3E8\n",
       "Name: vidid, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_vidid = input_data[input_column]\n",
    "temp_vidid.drop_duplicates(inplace=True)\n",
    "row_start=0 #change this\n",
    "proc_batch=100\n",
    "total_rows = len(temp_vidid) #end row, change this\n",
    "\n",
    "print(total_rows)\n",
    "\n",
    "# temp_res = extract_video_info(my_url[0])\n",
    "temp_vidid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '171527', 'msec')\n",
      "(64, '121803', 'msec')\n",
      "(128, '120660', 'msec')\n"
     ]
    }
   ],
   "source": [
    "fieldnames = ['title','keywords', 'author','category',\\\n",
    "              'viewcount','description','twitter_desc'\\\n",
    "              ,'channelID','duration','videoID']\n",
    "\n",
    "with open('temp_0.csv', 'wb') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile,delimiter='\\t',quotechar='\"',quoting=csv.QUOTE_NONE, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "for i in range(row_start,total_rows,proc_batch):\n",
    "    if(i>=row_start):\n",
    "        start_time = millis()\n",
    "\n",
    "        my_url = []\n",
    "        for i_sub in range(proc_batch):\n",
    "            temp = 'https://www.youtube.com/watch?v=%s' %(temp_vidid.iloc[i+i_sub])\n",
    "            my_url.append(temp)\n",
    "        results = [{} for x in my_url]\n",
    "\n",
    "        threads = []\n",
    "        # In this case 'urls' is a list of urls to be crawled.\n",
    "        for ii in range(len(my_url)):\n",
    "            # We start one thread per url present.\n",
    "            process = Thread(target=crawl, args=[my_url[ii], results, ii])\n",
    "            process.start()\n",
    "            threads.append(process)\n",
    "\n",
    "        # We now pause execution on the main thread by 'joining' all of our started threads.\n",
    "        # This ensures that each has finished processing the urls.\n",
    "        for process in threads:\n",
    "            process.join()\n",
    "\n",
    "        with open('temp_0.csv', 'ab') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile,delimiter='\\t', fieldnames=fieldnames)\n",
    "            for row in results:\n",
    "                writer.writerow(row)    \n",
    "        print(i,str(millis() - start_time), 'msec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark 0.10.4/ Spark 2.2.0",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
