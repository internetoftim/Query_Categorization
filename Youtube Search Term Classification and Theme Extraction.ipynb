{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Search Term Exploration\n",
    "#### By: Tim Santos \n",
    "<ztdsantos@globe.com.ph>\n",
    "<timothyisrael.santos@thinkbiganalytics.com>\n",
    "\n",
    "#### Background:\n",
    "- Present to AMP and identify potential use case and interest\n",
    "- To explore search term data and apply basic NLP techniques \n",
    "- To explore search term categorization\n",
    "\n",
    "Accommpanying presentation: https://goo.gl/cbC57D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import svm, metrics, linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV File of AMP-Labelled Data  to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=pd.read_csv('/data/raw_data/zted0040/amp_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>searchTerm</th>\n",
       "      <th>videoID</th>\n",
       "      <th>categoryID</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jeremi planez</td>\n",
       "      <td>--5u48IaR4M</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kundiman cho</td>\n",
       "      <td>--B791t__ok</td>\n",
       "      <td>22.0</td>\n",
       "      <td>People_and_Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art garfunkel the sound of silence</td>\n",
       "      <td>--DbgPXwLlM</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sowmoy</td>\n",
       "      <td>--GmYWoFyJ4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20r</td>\n",
       "      <td>--XVqynW3-M</td>\n",
       "      <td>22.0</td>\n",
       "      <td>People_and_Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           searchTerm      videoID  categoryID  \\\n",
       "0                       jeremi planez  --5u48IaR4M        10.0   \n",
       "1                        kundiman cho  --B791t__ok        22.0   \n",
       "2  art garfunkel the sound of silence  --DbgPXwLlM        10.0   \n",
       "3                              sowmoy  --GmYWoFyJ4        23.0   \n",
       "4                                 20r  --XVqynW3-M        22.0   \n",
       "\n",
       "           category  \n",
       "0             Music  \n",
       "1  People_and_Blogs  \n",
       "2             Music  \n",
       "3            Comedy  \n",
       "4  People_and_Blogs  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the CSV File of AMP-Labelled Data  to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_data=pd.read_csv('/data/raw_data/zted0040/summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adult</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autos_and_Vehicles</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Comedy</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "      <td>786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>3775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cluster_id  count\n",
       "0               Adult    374\n",
       "1  Autos_and_Vehicles    141\n",
       "2              Comedy    427\n",
       "3           Education    786\n",
       "4       Entertainment   3775"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the search term categories defined by AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult\n",
      "Autos_and_Vehicles\n",
      "Comedy\n",
      "Education\n",
      "Entertainment\n",
      "Film_and_Animation\n",
      "Gaming\n",
      "Howto_and_Style\n",
      "Movies\n",
      "Music\n",
      "News_and_Politics\n",
      "No_Category_Found\n",
      "Nonprofits_and_Activism\n",
      "People_and_Blogs\n",
      "Pets_and_Animals\n",
      "Science_and_Technology\n",
      "Shows\n",
      "Sports\n",
      "Trailers\n",
      "Travel_and_Events\n"
     ]
    }
   ],
   "source": [
    "for cat in summary_data.cluster_id:\n",
    "    print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some processing here\n",
    "\n",
    "- cleaning\n",
    "- parsing\n",
    "- stemming\n",
    "- lemmatization\n",
    "- POS-tagging\n",
    "- chunking\n",
    "- sentenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adata_parsed = adata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parsing (removed known html tags for youtube)\n",
    "adata_parsed.searchTerm=adata_parsed.searchTerm.str.replace('gl', '').str.replace('en', '').str.replace('hl', '')\n",
    "# adata_parsed=adata_parsed.searchTerm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>searchTerm</th>\n",
       "      <th>videoID</th>\n",
       "      <th>categoryID</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jeremi planez</td>\n",
       "      <td>--5u48IaR4M</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kundiman cho</td>\n",
       "      <td>--B791t__ok</td>\n",
       "      <td>22.0</td>\n",
       "      <td>People_and_Blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>art garfunkel the sound of silce</td>\n",
       "      <td>--DbgPXwLlM</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sowmoy</td>\n",
       "      <td>--GmYWoFyJ4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20r</td>\n",
       "      <td>--XVqynW3-M</td>\n",
       "      <td>22.0</td>\n",
       "      <td>People_and_Blogs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         searchTerm      videoID  categoryID          category\n",
       "0                     jeremi planez  --5u48IaR4M        10.0             Music\n",
       "1                      kundiman cho  --B791t__ok        22.0  People_and_Blogs\n",
       "2  art garfunkel the sound of silce  --DbgPXwLlM        10.0             Music\n",
       "3                            sowmoy  --GmYWoFyJ4        23.0            Comedy\n",
       "4                               20r  --XVqynW3-M        22.0  People_and_Blogs"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_parsed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize and Show top Unigram terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>love</td>\n",
       "      <td>0.018035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>lyrics</td>\n",
       "      <td>0.014737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>sm</td>\n",
       "      <td>0.009634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3581</th>\n",
       "      <td>song</td>\n",
       "      <td>0.008842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>like</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>ed</td>\n",
       "      <td>0.006292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>don</td>\n",
       "      <td>0.006055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2264</th>\n",
       "      <td>let</td>\n",
       "      <td>0.005519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3582</th>\n",
       "      <td>songs</td>\n",
       "      <td>0.005322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>sheeran</td>\n",
       "      <td>0.005234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         term    weight\n",
       "2359     love  0.018035\n",
       "2400   lyrics  0.014737\n",
       "3547       sm  0.009634\n",
       "3581     song  0.008842\n",
       "2290     like  0.006400\n",
       "1249       ed  0.006292\n",
       "1160      don  0.006055\n",
       "2264      let  0.005519\n",
       "3582    songs  0.005322\n",
       "3451  sheeran  0.005234"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search only Music category\n",
    "adata_parsed_temp=adata[adata.category=='Music'].searchTerm\n",
    "tvec = TfidfVectorizer(min_df=.0001, max_df=0.95,  stop_words='english', ngram_range=[1,1])\n",
    "tvec_weights = tvec.fit_transform(adata_parsed_temp)\n",
    "weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "temp_weights = weights_df.sort_values(by='weight', ascending=False).head(200)\n",
    "temp_weights.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the top Unigram to CSV (for visualization purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult.csv\n",
      "Adult.csv SAVED!\n",
      "Autos_and_Vehicles.csv\n",
      "Autos_and_Vehicles.csv SAVED!\n",
      "Comedy.csv\n",
      "Comedy.csv SAVED!\n",
      "Education.csv\n",
      "Education.csv SAVED!\n",
      "Entertainment.csv\n",
      "Entertainment.csv SAVED!\n",
      "Film_and_Animation.csv\n",
      "Film_and_Animation.csv SAVED!\n",
      "Gaming.csv\n",
      "Gaming.csv SAVED!\n",
      "Howto_and_Style.csv\n",
      "Howto_and_Style.csv SAVED!\n",
      "Movies.csv\n",
      "Movies.csv SAVED!\n",
      "Music.csv\n",
      "Music.csv SAVED!\n",
      "News_and_Politics.csv\n",
      "News_and_Politics.csv SAVED!\n",
      "No_Category_Found.csv\n",
      "No_Category_Found.csv SAVED!\n",
      "Nonprofits_and_Activism.csv\n",
      "Nonprofits_and_Activism.csv SAVED!\n",
      "People_and_Blogs.csv\n",
      "People_and_Blogs.csv SAVED!\n",
      "Pets_and_Animals.csv\n",
      "Pets_and_Animals.csv SAVED!\n",
      "Science_and_Technology.csv\n",
      "Science_and_Technology.csv SAVED!\n",
      "Shows.csv\n",
      "Shows.csv SAVED!\n",
      "Sports.csv\n",
      "Sports.csv SAVED!\n",
      "Trailers.csv\n",
      "Trailers.csv SAVED!\n",
      "Travel_and_Events.csv\n",
      "Travel_and_Events.csv SAVED!\n"
     ]
    }
   ],
   "source": [
    "for cat in summary_data.cluster_id:\n",
    "    \n",
    "    adata_parsed_temp=adata[adata.category==cat].searchTerm\n",
    "    tvec = TfidfVectorizer(min_df=.0001, max_df=0.95,  stop_words='english', ngram_range=[1,1])\n",
    "    tvec_weights = tvec.fit_transform(adata_parsed_temp)\n",
    "\n",
    "    weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "    temp_weights = weights_df.sort_values(by='weight', ascending=False).head(200)\n",
    "    temp_weights.weight=temp_weights.weight/temp_weights.weight.max()    \n",
    "    np_df = temp_weights.as_matrix()\n",
    "    print(cat+'.csv')\n",
    "\n",
    "    with open(cat+'_cnt.csv', 'wb') as f:\n",
    "        fs = ['cluster_id','type','word','count']\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fs)\n",
    "        for i in np_df:\n",
    "            writer.writerow([cat,'unigram',i[0],round(i[1]*100)])\n",
    "    print(cat+'_cnt.csv SAVED!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the actual search terms to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(cat+'_details.csv', 'wb') as f:\n",
    "    fs = ['cluster_id','paragraph']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(fs)\n",
    "    for i in adata_parsed_temp:\n",
    "        writer.writerow([cat,i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the top Bigram to CSV (for visualization purposes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult.csv\n",
      "Adult_cnt.csv SAVED!\n",
      "Autos_and_Vehicles.csv\n",
      "Autos_and_Vehicles_cnt.csv SAVED!\n",
      "Comedy.csv\n",
      "Comedy_cnt.csv SAVED!\n",
      "Education.csv\n",
      "Education_cnt.csv SAVED!\n",
      "Entertainment.csv\n",
      "Entertainment_cnt.csv SAVED!\n",
      "Film_and_Animation.csv\n",
      "Film_and_Animation_cnt.csv SAVED!\n",
      "Gaming.csv\n",
      "Gaming_cnt.csv SAVED!\n",
      "Howto_and_Style.csv\n",
      "Howto_and_Style_cnt.csv SAVED!\n",
      "Movies.csv\n",
      "Movies_cnt.csv SAVED!\n",
      "Music.csv\n",
      "Music_cnt.csv SAVED!\n",
      "News_and_Politics.csv\n",
      "News_and_Politics_cnt.csv SAVED!\n",
      "No_Category_Found.csv\n",
      "No_Category_Found_cnt.csv SAVED!\n",
      "Nonprofits_and_Activism.csv\n",
      "Nonprofits_and_Activism_cnt.csv SAVED!\n",
      "People_and_Blogs.csv\n",
      "People_and_Blogs_cnt.csv SAVED!\n",
      "Pets_and_Animals.csv\n",
      "Pets_and_Animals_cnt.csv SAVED!\n",
      "Science_and_Technology.csv\n",
      "Science_and_Technology_cnt.csv SAVED!\n",
      "Shows.csv\n",
      "Shows_cnt.csv SAVED!\n",
      "Sports.csv\n",
      "Sports_cnt.csv SAVED!\n",
      "Trailers.csv\n",
      "Trailers_cnt.csv SAVED!\n",
      "Travel_and_Events.csv\n",
      "Travel_and_Events_cnt.csv SAVED!\n"
     ]
    }
   ],
   "source": [
    "for cat in summary_data.cluster_id:\n",
    "    \n",
    "    adata_parsed_temp=adata[adata.category==cat].searchTerm\n",
    "    tvec = TfidfVectorizer(min_df=.0001, max_df=0.95,  stop_words='english', ngram_range=[2,2])\n",
    "    tvec_weights = tvec.fit_transform(adata_parsed_temp)\n",
    "\n",
    "    weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "    temp_weights = weights_df.sort_values(by='weight', ascending=False).head(200)\n",
    "    temp_weights.weight=temp_weights.weight/temp_weights.weight.max()    \n",
    "    np_df = temp_weights.as_matrix()\n",
    "    print(cat+'.csv')\n",
    "    with open(cat+'_cnt.csv', 'a') as f:\n",
    "        fs = ['cluster_id','type','word','count']\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fs)\n",
    "        for i in np_df:\n",
    "            writer.writerow([cat,'bigram',i[0],round(i[1]*100)])\n",
    "    print(cat+'_cnt.csv SAVED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the top Trigram to CSV (for visualization purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult.csv\n",
      "Adult_cnt.csv SAVED!\n",
      "Autos_and_Vehicles.csv\n",
      "Autos_and_Vehicles_cnt.csv SAVED!\n",
      "Comedy.csv\n",
      "Comedy_cnt.csv SAVED!\n",
      "Education.csv\n",
      "Education_cnt.csv SAVED!\n",
      "Entertainment.csv\n",
      "Entertainment_cnt.csv SAVED!\n",
      "Film_and_Animation.csv\n",
      "Film_and_Animation_cnt.csv SAVED!\n",
      "Gaming.csv\n",
      "Gaming_cnt.csv SAVED!\n",
      "Howto_and_Style.csv\n",
      "Howto_and_Style_cnt.csv SAVED!\n",
      "Movies.csv\n",
      "Movies_cnt.csv SAVED!\n",
      "Music.csv\n",
      "Music_cnt.csv SAVED!\n",
      "News_and_Politics.csv\n",
      "News_and_Politics_cnt.csv SAVED!\n",
      "No_Category_Found.csv\n",
      "No_Category_Found_cnt.csv SAVED!\n",
      "Nonprofits_and_Activism.csv\n",
      "Nonprofits_and_Activism_cnt.csv SAVED!\n",
      "People_and_Blogs.csv\n",
      "People_and_Blogs_cnt.csv SAVED!\n",
      "Pets_and_Animals.csv\n",
      "Pets_and_Animals_cnt.csv SAVED!\n",
      "Science_and_Technology.csv\n",
      "Science_and_Technology_cnt.csv SAVED!\n",
      "Shows.csv\n",
      "Shows_cnt.csv SAVED!\n",
      "Sports.csv\n",
      "Sports_cnt.csv SAVED!\n",
      "Trailers.csv\n",
      "Trailers_cnt.csv SAVED!\n",
      "Travel_and_Events.csv\n",
      "Travel_and_Events_cnt.csv SAVED!\n"
     ]
    }
   ],
   "source": [
    "for cat in summary_data.cluster_id:\n",
    "    \n",
    "    adata_parsed_temp=adata[adata.category==cat].searchTerm\n",
    "    tvec = TfidfVectorizer(min_df=.0001, max_df=0.95,  stop_words='english', ngram_range=[3,3])\n",
    "    tvec_weights = tvec.fit_transform(adata_parsed_temp)\n",
    "\n",
    "    weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n",
    "    weights_df = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "    temp_weights = weights_df.sort_values(by='weight', ascending=False).head(200)\n",
    "    temp_weights.weight=temp_weights.weight/temp_weights.weight.max()    \n",
    "    np_df = temp_weights.as_matrix()\n",
    "    print(cat+'.csv')\n",
    "    with open(cat+'_cnt.csv', 'a') as f:\n",
    "        fs = ['cluster_id','type','word','count']\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(fs)\n",
    "        for i in np_df:\n",
    "            writer.writerow([cat,'trigram',i[0],round(i[1]*100)])\n",
    "    print(cat+'_cnt.csv SAVED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Only Music Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_parsed_music=adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "adata_parsed_music.category[adata_parsed_music.category!='Music']=0\n",
    "adata_parsed_music.category[adata_parsed_music.category=='Music']=1\n",
    "# adata_parsed_music.category=adata_parsed_music.category.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_parsed_music.category=adata_parsed_music.category.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17365"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adata_parsed_music[adata_parsed_music.category==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16609"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adata_parsed_music[adata_parsed_music.category==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=adata_parsed_music.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Split Train-Test\n",
    "train, validate, test = np.split(adata_parsed_music.sample(frac=1), \n",
    "                                 [int(.6*len(adata_parsed_music)), \n",
    "                                  int(.8*len(adata_parsed_music))])\n",
    "## Extract Features:Term Frequency times Inverse Document Frequency (tf-idf)\n",
    "tfidf_transformer = TfidfVectorizer(min_df=.0001, max_df=0.95,  stop_words='english', ngram_range=[1,3])\n",
    "\n",
    "# Use transform() method to transform count-matrix to 'tf-idf' representation\n",
    "x_train_tfidf = tfidf_transformer.fit_transform(train['searchTerm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM/SGD Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = linear_model.SGDClassifier(loss='log')\n",
    "SG = clf.fit(x_train_tfidf.toarray(), train['category'])\n",
    "\n",
    "## Prediction on test data\n",
    "# Tokenizing test phrase\n",
    "# x_test_counts_ft_sel = count_vector_ft_sel.transform(test['searchTerm'])\n",
    "# Use transform() method to transform test count-matrix to 'tf-idf' representation\n",
    "# x_test_tfidf_ft_sel = tfidf_transformer_ft_sel.transform(x_test_counts_ft_sel)\n",
    "x_test_tfidf = tfidf_transformer.transform(test['searchTerm'])\n",
    "\n",
    "predicted=SG.predict(x_test_tfidf)\n",
    "\n",
    "    \n",
    "test['predicted']=predicted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Confusion Matrix for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |    0    1    2 |\n",
      "--+----------------+\n",
      "0 |<1629>1712    . |\n",
      "1 |  365<3089>   . |\n",
      "2 |    .    .   <.>|\n",
      "--+----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.49      0.61      3341\n",
      "          1       0.64      0.89      0.75      3454\n",
      "\n",
      "avg / total       0.73      0.69      0.68      6795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(test['category'], predicted)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "print(classification_report(test['category'], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling\n",
    "\n",
    "Perform theme extraction uusing LDA to discover underlying themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features using Count Vectorizer\n",
    "For LDA and NMF, frequency counts will suffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adata_parsed=adata\n",
    "# Some basic parsing (removed known html tags for youtube)\n",
    "adata_parsed.searchTerm=adata_parsed.searchTerm.str.replace('gl', '').str.replace('en', '').str.replace('hl', '')\n",
    "# adata_parsed=adata_parsed.searchTerm.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfvec = CountVectorizer(min_df=.0001, max_df=0.95,  stop_words='english', ngram_range=[2,2])\n",
    "tfvec_weights = tfvec.fit_transform(adata_parsed.searchTerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=100000, stop_words='english', ngram_range=[2,2])\n",
    "tfidf = tfidf_vectorizer.fit_transform(adata_parsed.searchTerm)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=100000, stop_words='english', ngram_range=[2,2])\n",
    "tf = tf_vectorizer.fit_transform(adata_parsed.searchTerm)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tvec_weights)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tfvec_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Helper Function to display topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display LDA Topics and Corresponding Top Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "2015 sm 42 maryzark 33 miles 30 sm 30 seconds 80 e0 80 e6 20 sorry 20 scarlet 20 sm\n",
      "Topic 1:\n",
      "81 d9 83 88 83 87 2016 best 20 sa 20 sarah 20 scarlet 20 sm 20 soapdish 83 ad\n",
      "Topic 2:\n",
      "83 81 81 97 21 guns 25 2017 80 99t 83 a1 83 ad 20 soapdish 20 sarah 20 scarlet\n",
      "Topic 3:\n",
      "81 e3 2016 best 83 8a 83 ad 20 soapdish 20 sabihin 20 sarah 20 scarlet 20 sm 20 sorry\n",
      "Topic 4:\n",
      "22 2015 70 80 83 ad 20 steve 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 sub\n",
      "Topic 5:\n",
      "83 96 2x faster 20 curse 60 70 83 ad 20 steve 20 scarlet 20 sm 20 soapdish 20 sorry\n",
      "Topic 6:\n",
      "82 92 21 21 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve 20 sorry\n",
      "Topic 7:\n",
      "81 84 20 yohan 16 latest 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sub 20 sud\n",
      "Topic 8:\n",
      "80 eb 83 83 83 ad 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sa\n",
      "Topic 9:\n",
      "20 sarah 2013 sm 80 93 83 ad 20 sabihin 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud\n",
      "Topic 10:\n",
      "16 sm 81 8a 83 ad 20 sabihin 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub\n",
      "Topic 11:\n",
      "23 c2017 81 82 83 ad 20 sorry 20 sarah 20 scarlet 20 sm 20 soapdish 20 steve 20 sa\n",
      "Topic 12:\n",
      "2013 season 20 dawin 09 tracer 83 ad 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sud\n",
      "Topic 13:\n",
      "21 c2017 20 little 22 sm 80 disco 82 ab 20 sud 20 sub 20 tadhana 20 tagalog 20 steve\n",
      "Topic 14:\n",
      "80 b1 83 ad 20 touch 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve\n",
      "Topic 15:\n",
      "21 rude 83 ad 20 sub 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sud\n",
      "Topic 16:\n",
      "82 93 81 8c 83 ad 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve 20 sorry\n",
      "Topic 17:\n",
      "50 ct 2016 sm 83 ad 20 steve 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 sub\n",
      "Topic 18:\n",
      "83 ac 20 wag 20 took 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve\n",
      "Topic 19:\n",
      "82 b0 83 ad 20 touch 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(nmf, tfidf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display NMF Topics and Corresponding Top Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "2015 sm 42 maryzark 33 miles 30 sm 30 seconds 80 e0 80 e6 20 sorry 20 scarlet 20 sm\n",
      "Topic 1:\n",
      "81 d9 83 88 83 87 2016 best 20 sa 20 sarah 20 scarlet 20 sm 20 soapdish 83 ad\n",
      "Topic 2:\n",
      "83 81 81 97 21 guns 25 2017 80 99t 83 a1 83 ad 20 soapdish 20 sarah 20 scarlet\n",
      "Topic 3:\n",
      "81 e3 2016 best 83 8a 83 ad 20 soapdish 20 sabihin 20 sarah 20 scarlet 20 sm 20 sorry\n",
      "Topic 4:\n",
      "22 2015 70 80 83 ad 20 steve 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 sub\n",
      "Topic 5:\n",
      "83 96 2x faster 20 curse 60 70 83 ad 20 steve 20 scarlet 20 sm 20 soapdish 20 sorry\n",
      "Topic 6:\n",
      "82 92 21 21 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve 20 sorry\n",
      "Topic 7:\n",
      "81 84 20 yohan 16 latest 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sub 20 sud\n",
      "Topic 8:\n",
      "80 eb 83 83 83 ad 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sa\n",
      "Topic 9:\n",
      "20 sarah 2013 sm 80 93 83 ad 20 sabihin 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud\n",
      "Topic 10:\n",
      "16 sm 81 8a 83 ad 20 sabihin 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub\n",
      "Topic 11:\n",
      "23 c2017 81 82 83 ad 20 sorry 20 sarah 20 scarlet 20 sm 20 soapdish 20 steve 20 sa\n",
      "Topic 12:\n",
      "2013 season 20 dawin 09 tracer 83 ad 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sud\n",
      "Topic 13:\n",
      "21 c2017 20 little 22 sm 80 disco 82 ab 20 sud 20 sub 20 tadhana 20 tagalog 20 steve\n",
      "Topic 14:\n",
      "80 b1 83 ad 20 touch 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve\n",
      "Topic 15:\n",
      "21 rude 83 ad 20 sub 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 steve 20 sud\n",
      "Topic 16:\n",
      "82 93 81 8c 83 ad 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve 20 sorry\n",
      "Topic 17:\n",
      "50 ct 2016 sm 83 ad 20 steve 20 sarah 20 scarlet 20 sm 20 soapdish 20 sorry 20 sub\n",
      "Topic 18:\n",
      "83 ac 20 wag 20 took 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve\n",
      "Topic 19:\n",
      "82 b0 83 ad 20 touch 20 tj 20 tanging 20 tagalog 20 tadhana 20 sud 20 sub 20 steve\n",
      "Topic 0:\n",
      "9c ec constantino paasa catch breath asin masdan cheap thrills bts today bell rock d9 82 d8 b4 broke fracer\n",
      "Topic 1:\n",
      "dela cerna auli cravalho ahon sev daryl ong cold play colors wind bc e3 amazing world aramina xxx asian box\n",
      "Topic 2:\n",
      "boyce avue danthology 2016 celebrity sex cruisin love dancing que d7 99 ako ogie congratulations post acute kidney dahan lang\n",
      "Topic 3:\n",
      "camila cabello cher klosner bikini boy chiu sing cesar montano connie francis defying gravity akuztikz sa buhay ko 9c 9d\n",
      "Topic 4:\n",
      "beauty beast disney princess a9 d7 a5 e0 ba ang crochet bracelet bed chris bea miller big fat a8 e7\n",
      "Topic 5:\n",
      "daughtry waiting banana nite bad massage ba siya argtina olympic afraid love 83 84 9c ed air suply best korean\n",
      "Topic 6:\n",
      "83 a4 cheer leader 13 c2017 diwata abra air supply cha cha ali dance die multiply 99 80 a8 eb\n",
      "Topic 7:\n",
      "acoustic page 2017 songs ac ec 2016 best bubble balloon ad yo achy breaky 21 c2017 2016 songs 21 rude\n",
      "Topic 8:\n",
      "d7 a0 93 d1 99 perc ace base 2016 sm adam lambert chasing cars bakit ba 12 51 access pc\n",
      "Topic 9:\n",
      "anna kdrick danza kuduro delhi minister basketball finals boys sm 83 d8 b9 d9 baby movies asian porn day life\n",
      "Topic 10:\n",
      "bruno mara best dance direction story bridge troubled cool kids birthday surprise 83 89 darker page basa sa california king\n",
      "Topic 11:\n",
      "break free d8 ac ah joong arabic songs cannon rock 81 aa bugoy drilon 80 disco dixie chicks crazy moffatts\n",
      "Topic 12:\n",
      "andy grammer androwe song andrew movies cream cake boys flowers bo gu akala mo 20 love 85 d8 20 kanlungan\n",
      "Topic 13:\n",
      "a7 d9 darker zayn d8 b8 bukas movie cantadia 05 a4 ae carpters songs battle 2017 bamboo music c3 a9line\n",
      "Topic 14:\n",
      "curtis chapman d1 83 bites dust 98 d7 bata pa criss angel d8 b1 anime basketball alright justin avue say\n",
      "Topic 15:\n",
      "adam mitchell 81 a7 carey baby ae a4 27 2017 ae d9 25 minutes ae e3 aegis aegis adele lyrics\n",
      "Topic 16:\n",
      "aerobic dance acoustic version 8a d8 case acer akoy bata beautiful soul darr jed camp sawi dise laurel deep love\n",
      "Topic 17:\n",
      "body attack d7 95 direction lyrics apoc vs breaky heart ava nursery chainsmokers coldplay dan stevs cube tutorial a1 heav\n",
      "Topic 18:\n",
      "boys like a0 d7 direction night chained rhythm beauty beat austin mahone cheerleader felix 95 99 away frids birthday spanish\n",
      "Topic 19:\n",
      "die trying 82 a0 anak ng b1 e5 cravalho far 93 e3 a8 d8 collin raye 98 9f buli buli\n"
     ]
    }
   ],
   "source": [
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
